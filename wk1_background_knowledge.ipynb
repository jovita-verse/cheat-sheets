{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About the datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Datasets**\n",
    "\n",
    "1. **Training set**: Used to train the model.  \n",
    "2. **Validation set**: Used to tune hyperparameters and monitor training.  \n",
    "3. **Test set**: Used *only once* for a final unbiased evaluation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tactics**\n",
    "1. **Data Splitting**:\n",
    "   - Used `random_split` to create a validation set from the training data (10% by default).  \n",
    "   - **Test set remains untouched** until the final evaluation.  \n",
    "\n",
    "2. **Three Phases**:\n",
    "   - **Training**: Updates model weights.  \n",
    "   - **Validation**: Monitors performance for early stopping/hyperparameter tuning.  \n",
    "   - **Test**: *Only used once* at the end for a final unbiased metric.  \n",
    "\n",
    "3. **Metrics**:\n",
    "   - Tracked separately for train/val/test.\n",
    "   - Accuracy tracking computed using `torch.max(outputs, 1)` for both training and validation.\n",
    "\n",
    "4. **Early Stopping**:\n",
    "   - Stop training if validation loss doesnâ€™t improve for N epochs.\n",
    "\n",
    "5. **TensorBoard Logging**:\n",
    "   - Use `torch.utils.tensorboard` or `Weights & Biases` for advanced tracking.\n",
    "   - Logs loss and accuracy curves.\n",
    "   ```\n",
    "6. **Matplotlib Plots**:\n",
    "   - Dual plots for loss and accuracy (side-by-side).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Matters**\n",
    "- **Prevents Data Leakage**: The test set simulates \"unseen real-world data.\" Using it for validation would inflate perceived performance.  \n",
    "- **Better Generalization**: Hyperparameters tuned on the validation set (not test set) ensure the model adapts to *true* unseen data.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Each Set**\n",
    "| **Phase**      | **Purpose**                                | **When to Use**                |\n",
    "|----------------|-------------------------------------------|--------------------------------|\n",
    "| **Training**   | Fit model parameters                      | Always                         |\n",
    "| **Validation** | Tune hyperparameters, early stopping      | During training                |\n",
    "| **Test**       | Final evaluation (unbiased)               | *Only once* after all tuning   |\n",
    "\n",
    "---\n",
    "\n",
    "### **Further Customization**\n",
    "- **Hyperparameter Tuning**: Use `torch.optim.lr_scheduler`.\n",
    "- **Data Augmentation**: Add more transforms (e.g., `RandomHorizontalFlip()`).\n",
    "- **Model Checkpoints**: Save all epochs with `torch.save(checkpoint, f'epoch_{epoch}.pth')`.\n",
    "\n",
    "Let me know if you'd like to add more features! ðŸš€\n",
    "\n",
    "This is the gold-standard workflow for robust model development. Let me know if you'd like to add more (e.g., cross-validation)! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Saving a model after training and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a trained PyTorch model is essential for reuse, deployment, or further training.\n",
    "\n",
    "\n",
    "### **1. Key Notes**\n",
    "\n",
    "- **Formats**:  \n",
    "  - `.pth` or `.pt` are common PyTorch extensions.  \n",
    "- **What to Save**:  \n",
    "  - For inference: Only `state_dict` is sufficient.  \n",
    "  - For resuming training: Save optimizer state + epoch.  \n",
    "- **Device Handling**:  \n",
    "  - If loading a model trained on GPU to CPU:  \n",
    "    ```python\n",
    "    checkpoint = torch.load(\"model.pth\", map_location=torch.device('cpu'))\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Best Practices**\n",
    "Best practices are the following:  \n",
    "- Saving the **model architecture + trained weights**.  \n",
    "- Saving only the **state_dict** (recommended for production).  \n",
    "- Loading the model later for inference.  \n",
    "\n",
    "#### 1. Saving the Model (Full Model or State Dict)\n",
    "\n",
    "Add this code after your training/evaluation loop:\n",
    "\n",
    "[code already in model_building file]\n",
    "\n",
    "#### 2. Loading the Model Later\n",
    "\n",
    "*Option A: Load Full Model (if saved with `torch.save(model)`)*\n",
    "\n",
    "[code already in model_building file]\n",
    "\n",
    "*Option B: Load State Dict (recommended)*\n",
    "\n",
    "[code already in model_building file]\n",
    "\n",
    "#### 3. Example: Save the Best Model (During Training)\n",
    "\n",
    "Only save when validation accuracy improves:\n",
    "```python\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # ... (training loop code) ...\n",
    "\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # ... (evaluation code) ...\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Save only if best accuracy improves\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "        print(f\"Saved new best model with accuracy: {accuracy:.2f}%\")\n",
    "```\n",
    "\n",
    "#### *5. Verify Loaded Model*\n",
    "\n",
    "```python\n",
    "# Test the loaded model\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = loaded_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Loaded Model Accuracy: {100 * correct / len(test_data)}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which Method?**\n",
    "| **Scenario**               | **Recommended Save Method**          |\n",
    "|----------------------------|--------------------------------------|\n",
    "| Deploying for inference    | `model.state_dict()`                 |\n",
    "| Resuming training later    | Checkpoint (model + optimizer)       |\n",
    "| Sharing with others        | Full model (`torch.save(model)`)     |\n",
    "\n",
    "Let me know if you'd like help adapting this to a specific use case! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to use TensorBoard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s the **extended version** with **accuracy tracking**, **early stopping**, and **TensorBoard logging** (alongside matplotlib plots).\n",
    "\n",
    "Using **TensorBoard** in a Jupyter Notebook is straightforward once you know the steps. Below is a **clear, step-by-step guide** with code examples to visualize your PyTorch training metrics (loss, accuracy, etc.) directly in the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step: TensorBoard in Jupyter Notebook**\n",
    "#### 1. **Install TensorBoard**\n",
    "If you havenâ€™t already:\n",
    "```bash\n",
    "pip install tensorboard\n",
    "```\n",
    "\n",
    "#### 2. **Set Up TensorBoard in Your Notebook**\n",
    "Run these commands **in your Jupyter notebook** to load the TensorBoard extension and start logging:\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs  # Auto-opens TensorBoard (if running locally)\n",
    "```\n",
    "\n",
    "#### 3. **Modify Your Training Code to Log Metrics**\n",
    "Add TensorBoard logging to your PyTorch training loop. Hereâ€™s the key part:\n",
    "\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize writer (saves logs to 'runs/' directory)\n",
    "writer = SummaryWriter('runs/experiment_1')  # Change 'experiment_1' for each run\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ... Training code ...\n",
    "    train_loss = ...\n",
    "    val_accuracy = ...\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "# Close the writer when done\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "#### 4. **View TensorBoard in Jupyter**\n",
    "After running the training loop, refresh TensorBoard to see plots:\n",
    "```python\n",
    "%tensorboard --logdir=runs\n",
    "```\n",
    "- **Expected Output**:  \n",
    "  *(Shows loss/accuracy curves over epochs)*\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Notes**\n",
    "1. **Where Logs Are Saved**:  \n",
    "   - Logs are saved in the `runs/` directory (created automatically).  \n",
    "   - Use different subdirectories (e.g., `runs/exp1`, `runs/exp2`) to compare experiments.\n",
    "\n",
    "2. **What You Can Log**:\n",
    "   ```python\n",
    "   writer.add_scalar('Metric_Name', value, epoch)  # Scalars (loss, accuracy)\n",
    "   writer.add_graph(model, input_tensor)           # Model architecture\n",
    "   writer.add_images('input_images', images)       # Images (for vision tasks)\n",
    "   ```\n",
    "\n",
    "3. **If TensorBoard Doesnâ€™t Load**:\n",
    "   - Check the log directory path matches `--logdir`.  \n",
    "   - Restart the notebook kernel if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Full Example: Training Loop with TensorBoard**\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize\n",
    "writer = SummaryWriter('runs/cifar10_cnn')\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_accuracy = 100 * val_accuracy / len(val_data)\n",
    "    writer.add_scalar('Accuracy/val', avg_val_accuracy, epoch)\n",
    "\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Troubleshooting**\n",
    "- **Error: \"No dashboards are active\"**:  \n",
    "  - Ensure `writer.close()` isnâ€™t called too early.  \n",
    "  - Verify the log directory path is correct in `%tensorboard --logdir=runs`.\n",
    "\n",
    "- **Clear Old Logs**:  \n",
    "  Delete the `runs/` folder or use a new subdirectory to avoid clutter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced: Compare Multiple Experiments**\n",
    "```python\n",
    "# Experiment 1\n",
    "writer1 = SummaryWriter('runs/lr_0.01')\n",
    "# ... train model with lr=0.01 ...\n",
    "\n",
    "# Experiment 2\n",
    "writer2 = SummaryWriter('runs/lr_0.001')\n",
    "# ... train model with lr=0.001 ...\n",
    "```\n",
    "Then compare in TensorBoard:\n",
    "```python\n",
    "%tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Now you can visualize and compare training dynamics interactively! Let me know if you hit any snags. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Background Knowledge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! Yes, I have extensive programming knowledge, including deep learning and PyTorch. Since you're already familiar with Keras, TensorFlow, and Scikit-learn, transitioning to PyTorch should be smoothâ€”it's a powerful and flexible library, especially for research-oriented work.  \n",
    "\n",
    "## **PyTorch for Image Classification**  \n",
    "PyTorch is known for its dynamic computation graph (`autograd`), which makes debugging easier compared to TensorFlow's static graphs (though TF 2.x also uses eager execution by default). Hereâ€™s a structured approach to help you get started:\n",
    "\n",
    "---\n",
    "\n",
    "## **A. Basics**\n",
    "### **1. Key PyTorch Concepts to Learn**\n",
    "- **Tensors**: PyTorchâ€™s equivalent of NumPy arrays (but with GPU support).  \n",
    "- **Autograd**: Automatic differentiation for gradient computation.  \n",
    "- **nn.Module**: Base class for all neural network architectures.  \n",
    "- **DataLoader & Dataset**: For efficient batch loading and preprocessing.  \n",
    "- **Optimizers (SGD, Adam)**: Similar to Keras but with more flexibility.  \n",
    "- **Device Management (CPU/GPU)**: Moving tensors/models with `.to(device)`.\n",
    "\n",
    "### **2. Key Differences from Keras/TensorFlow**\n",
    "| **PyTorch** | **Keras/TensorFlow** |\n",
    "|-------------|----------------------|\n",
    "| Explicit GPU management (`.to(device)`) | Automatic with `tf.distribute` |\n",
    "| Dynamic computation graphs | Static graphs (TF 1.x) / Eager (TF 2.x) |\n",
    "| More low-level control | Higher-level abstractions |\n",
    "| `nn.Module` for models | `tf.keras.Model` or `Sequential` |\n",
    "\n",
    "### **3. Accuracy of This Model**  \n",
    "The accuracy of the CNN model I provided will depend on several factors, including:  \n",
    "- **Number of epochs** (I used 10, but more may improve accuracy).  \n",
    "- **Model architecture** (this is a simple 2-layer CNNâ€”deeper networks like ResNet perform better).  \n",
    "- **Hyperparameters** (learning rate, batch size, optimizer).  \n",
    "- **Dataset** (CIFAR-10 has 60k images, but is harder than MNIST).  \n",
    "\n",
    "### **Expected Values**  \n",
    "With the given code (2 conv layers + 2 linear layers, 10 epochs), the accuracy on **CIFAR-10** typically ranges between **~65% to 75%** on the test set. For comparison:  \n",
    "- A basic CNN in Keras/TensorFlow would yield similar results.  \n",
    "- State-of-the-art models (e.g., ResNet-18) achieve **>90%** on CIFAR-10.  \n",
    "\n",
    "### **4. Advanced Topics to Explore**\n",
    "- **Transfer Learning**: Use `torchvision.models` (e.g., ResNet, VGG).  \n",
    "- **Custom Datasets**: Subclass `torch.utils.data.Dataset`.  \n",
    "- **Mixed Precision Training**: `torch.cuda.amp` for speedup.  \n",
    "- **Distributed Training**: `torch.nn.DataParallel` or `DistributedDataParallel`.  \n",
    "\n",
    "### **5. Resources**\n",
    "- [PyTorch Official Tutorials](https://pytorch.org/tutorials/)  \n",
    "- [Deep Learning with PyTorch (Book)](https://pytorch.org/deep-learning-with-pytorch)  \n",
    "- [Fast.ai Course (Practical Approach)](https://course.fast.ai/)  \n",
    "\n",
    "---\n",
    "\n",
    "## **B. Methods for Increasing Accuracy**  \n",
    "1. **Deeper Architecture**:  \n",
    "   - Add more conv layers (e.g., 3â€“4 blocks with BatchNorm).  \n",
    "   - Use residual connections (ResNet).\n",
    "\n",
    "2. **Regularization**:  \n",
    "   - Add **Dropout** (`nn.Dropout(0.2)` after linear layers).  \n",
    "   - **Data Augmentation** (random crops, flips):  \n",
    "     ```python\n",
    "     transform_train = transforms.Compose([\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     ])\n",
    "     ```\n",
    "\n",
    "3. **Hyperparameter Tuning**:  \n",
    "   - Try a smaller learning rate (e.g., `lr=0.0001`) or scheduler (`torch.optim.lr_scheduler.StepLR`).  \n",
    "   - Increase batch size (e.g., 64 or 128) if GPU memory allows.  \n",
    "\n",
    "4. **Transfer Learning**:  \n",
    "   ```python\n",
    "   model = torchvision.models.resnet18(pretrained=True)\n",
    "   model.fc = nn.Linear(512, 10)  # Replace final layer for CIFAR-10\n",
    "   ```\n",
    "\n",
    "### **Example of an Improved Model (~85% Accuracy)**  \n",
    "```python\n",
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "**Changes**:  \n",
    "1. Added 3rd conv layer.  \n",
    "2. Introduced Dropout.  \n",
    "3. Better feature extraction (128 channels).  \n",
    "\n",
    "### **Final Notes**  \n",
    "- Always monitor **training/validation loss** to detect overfitting.  \n",
    "- Use **TensorBoard** or **Weights & Biases** for logging.  \n",
    "- PyTorch Lightning simplifies boilerplate code if you prefer higher-level APIs.  \n",
    "\n",
    "Would you like me to help you fine-tune this model further? ðŸŽ¯\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolution layer, pooling layer, batch normalisation layer, activation layer, dropout layer: in which order should these layers appear in a neural network architecture?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of layers in a neural network is crucial for stable training and effective feature extraction. Hereâ€™s the **standard order** for layers in a convolutional neural network (CNN), along with explanations and PyTorch code examples:\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommended Order for CNN Layers**\n",
    "1. **Convolution Layer**  \n",
    "   - Extracts spatial features (e.g., edges, textures).  \n",
    "2. **Batch Normalization (Optional)**  \n",
    "   - Normalizes activations to stabilize training (usually after conv/before activation).  \n",
    "3. **Activation Layer**  \n",
    "   - Introduces non-linearity (e.g., ReLU, LeakyReLU).  \n",
    "4. **Pooling Layer**  \n",
    "   - Downsamples spatial dimensions (e.g., MaxPool2d).  \n",
    "5. **Dropout (Optional)**  \n",
    "   - Regularization to prevent overfitting (common *after* pooling or dense layers).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Typical PyTorch Code Structure**\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Convolution\n",
    "            nn.BatchNorm2d(32),                           # BatchNorm\n",
    "            nn.ReLU(),                                    # Activation\n",
    "            nn.MaxPool2d(2),                              # Pooling\n",
    "            nn.Dropout(0.25)                              # Dropout (optional)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 8 * 8, 10)              # Fully Connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Rules & Exceptions**\n",
    "1. **BatchNorm Placement**:  \n",
    "   - Usually placed **after Conv/Linear layers but before activation** (ReLU).  \n",
    "   - Why? Normalizes inputs to the activation, reducing internal covariate shift.  \n",
    "\n",
    "2. **Dropout Placement**:  \n",
    "   - Common **after pooling** or **before dense layers** (not always after every conv).  \n",
    "   - Avoid using Dropout right before BatchNorm (they can conflict).  \n",
    "\n",
    "3. **Pooling Placement**:  \n",
    "   - Typically follows **Conv â†’ BatchNorm â†’ Activation**.  \n",
    "\n",
    "4. **Activation Choice**:  \n",
    "   - ReLU is default, but alternatives like **LeakyReLU** or **Swish** may work better.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Deviate from the Standard Order**\n",
    "- **Pre-Activation ResNets**: Place BatchNorm and ReLU *before* Conv layers.  \n",
    "  ```python\n",
    "  nn.Sequential(\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "  )\n",
    "  ```\n",
    "- **No BatchNorm**: For small datasets or simple models, omit BatchNorm (use Dropout instead).  \n",
    "- **Dropout in Conv Layers**: Rare, but can be used with very deep networks (e.g., 0.1â€“0.2 dropout).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of a CNN Block**\n",
    "```\n",
    "Input â†’ [Conv â†’ BatchNorm â†’ ReLU â†’ Pooling â†’ Dropout] â†’ ... â†’ Flatten â†’ Dense Layers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Order Works**\n",
    "1. **Convolution First**: Feature extraction starts with raw input.  \n",
    "2. **BatchNorm Before Activation**: Ensures normalized inputs to ReLU.  \n",
    "3. **Pooling After Activation**: Retains the most activated features.  \n",
    "4. **Dropout Last**: Regularizes after feature aggregation (pooling).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Full CNN with All Layers**\n",
    "```python\n",
    "class AdvancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Notes**\n",
    "- **Test Combinations**: Order can vary based on the problem (e.g., BatchNorm after ReLU sometimes works).  \n",
    "- **Monitor Training**: Use TensorBoard to check if your layer order causes instability.  \n",
    "\n",
    "Let me know if youâ€™d like to experiment with a specific architecture! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Comparing pre-trained models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below are concise yet insightful 200-word descriptions of each neural network, covering their history, datasets, strengths, and relevance in computer vision.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. AlexNet**  \n",
    "**History & Datasets**: AlexNet, introduced in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, was the breakthrough CNN that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with a top-5 error rate of 15.3%, nearly halving the previous best. Trained on ImageNet (1.2M images, 1K classes), it popularized deep learning by proving GPUs could accelerate training.  \n",
    "\n",
    "**Strengths & Relevance**: AlexNet introduced ReLU activations, dropout layers, and overlapping pooling, setting foundational practices still used today. While now outdated, its impact is undeniableâ€”it proved deep CNNs could outperform traditional methods.  \n",
    "\n",
    "**Potential**: Though surpassed by modern architectures, AlexNet remains a benchmark for historical significance and educational purposes, illustrating how simple but well-structured CNNs can achieve remarkable results.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. ConvNeXt**  \n",
    "**History & Datasets**: Developed by Facebook AI in 2022, ConvNeXt modernized traditional CNNs by borrowing concepts from Vision Transformers (ViTs). Trained on ImageNet-1K and ImageNet-22K, it outperforms ViTs while maintaining CNN efficiency.  \n",
    "\n",
    "**Strengths & Relevance**: By incorporating depthwise convolutions, LayerNorm, and inverted bottlenecks, ConvNeXt achieves ViT-like accuracy with lower computational costs. Itâ€™s particularly strong in high-resolution tasks where pure ViTs struggle.  \n",
    "\n",
    "**Potential**: ConvNeXt bridges the gap between CNNs and transformers, making it ideal for applications needing both efficiency and high performance, such as medical imaging or satellite analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. DenseNet**  \n",
    "**History & Datasets**: Proposed in 2017 by Gao Huang et al., DenseNet introduced dense connectivityâ€”each layer connects to all subsequent layersâ€”improving gradient flow and feature reuse. Trained on ImageNet and CIFAR, it outperformed ResNet with fewer parameters.  \n",
    "\n",
    "**Strengths & Relevance**: Its dense blocks mitigate vanishing gradients, making training deeper networks easier. DenseNet is compact yet powerful, ideal for scenarios with limited data.  \n",
    "\n",
    "**Potential**: Still relevant in specialized tasks like medical imaging, where feature preservation and model efficiency are critical.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. EfficientNetV2**  \n",
    "**History & Datasets**: Googleâ€™s 2021 upgrade to EfficientNet, using neural architecture search (NAS) to optimize speed and accuracy. Trained on ImageNet and JFT-300M (larger variants).  \n",
    "\n",
    "**Strengths & Relevance**: Introduces progressive learning (smaller images early, scaling up) and fused-MBConv layers, making it faster and more accurate than V1.  \n",
    "\n",
    "**Potential**: Perfect for edge AI, where balancing latency and accuracy is keyâ€”think real-time mobile vision apps.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. GoogLeNet (InceptionV1)**  \n",
    "**History & Datasets**: The 2014 ILSVRC winner (6.7% top-5 error), GoogLeNet introduced Inception modulesâ€”parallel convolutions at multiple scales. Trained on ImageNet.  \n",
    "\n",
    "**Strengths & Relevance**: Its 1x1 convolutions reduce parameters dramatically while capturing multi-scale features.  \n",
    "\n",
    "**Potential**: Though surpassed by later Inception versions, its design philosophy influences modern multi-branch architectures.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. InceptionV3**  \n",
    "**History & Datasets**: A 2015 upgrade, refining GoogLeNet with factorized convolutions (e.g., 3x3 â†’ 1x3 + 3x1) and label smoothing. Trained on ImageNet.  \n",
    "\n",
    "**Strengths & Relevance**: More efficient than V1, with better regularization.  \n",
    "\n",
    "**Potential**: Still a solid choice for mid-resource systems needing a balance of speed and accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. MaxViT**  \n",
    "**History & Datasets**: A 2022 hybrid (CNN + Transformer) by Google, merging local (CNN) and global (ViT) attention. Trained on ImageNet.  \n",
    "\n",
    "**Strengths & Relevance**: Grid-based attention reduces ViTâ€™s computational overhead while maintaining global context.  \n",
    "\n",
    "**Potential**: A future-proof choice for tasks needing both fine details and broad context, like autonomous driving.  \n",
    "\n",
    "---\n",
    "\n",
    "### **8. MNASNet**  \n",
    "**History & Datasets**: Googleâ€™s 2018 mobile-optimized model via NAS. Trained on ImageNet.  \n",
    "\n",
    "**Strengths & Relevance**: Pareto-optimal for latency-accuracy tradeoffs (1.5x faster than MobileNetV2 on phones).  \n",
    "\n",
    "**Potential**: Ideal for real-time mobile apps (e.g., live video filters).  \n",
    "\n",
    "---\n",
    "\n",
    "### **9. MobileNetV2**  \n",
    "**History & Datasets**: Googleâ€™s 2018 lightweight model for edge devices. Trained on ImageNet and COCO.  \n",
    "\n",
    "**Strengths & Relevance**: Uses inverted residuals for efficiency (<10ms inference on phones).  \n",
    "\n",
    "**Potential**: Dominates mobile/embedded vision (drones, wearables).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thoughts**  \n",
    "Each network has carved its nicheâ€”from AlexNetâ€™s legacy to MobileNetV2â€™s edge dominance. ConvNeXt and MaxViT represent the future, blending CNNs and transformers, while DenseNet and EfficientNetV2 offer specialized efficiencies. Choose based on your taskâ€™s demands: raw accuracy (ConvNeXt), speed (MobileNetV2), or a hybrid approach (MaxViT). ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ResNet for fashion MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a **pretrained model** in PyTorch for **Fashion MNIST** classification is straightforward, even though most pretrained models (like ResNet, EfficientNet) are trained on **ImageNet** (3-channel RGB images), while Fashion MNIST uses **grayscale (1-channel)** images. Hereâ€™s a step-by-step guide to adapt a pretrained model for Fashion MNIST:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Install PyTorch and TorchVision**\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Load and Preprocess Fashion MNIST**\n",
    "Fashion MNIST is included in `torchvision.datasets`. Weâ€™ll:\n",
    "- Convert grayscale to 3-channel (to match pretrained models).\n",
    "- Normalize using ImageNet stats (required for pretrained models).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel\n",
    "    transforms.Resize(224),  # Pretrained models typically expect 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Load a Pretrained Model**\n",
    "Weâ€™ll use **ResNet18** (lightweight and effective) and modify its final layer for Fashion MNISTâ€™s 10 classes.\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers (optional for transfer learning)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, 10)  # Fashion MNIST has 10 classes\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Train the Model**\n",
    "Since weâ€™re using a pretrained model, we only train the final layer (or fine-tune deeper layers if needed).\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):  # Train for 5 epochs\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Evaluate on Test Data**\n",
    "```python\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations**\n",
    "1. **Input Channels**:  \n",
    "   - Fashion MNIST is grayscale (1-channel), but pretrained models expect 3-channel RGB. We use `Grayscale(num_output_channels=3)` to replicate the single channel across RGB.\n",
    "\n",
    "2. **Transfer Learning Strategies**:  \n",
    "   - **Feature Extraction**: Freeze all layers (as above) and only train the new `fc` layer.  \n",
    "   - **Fine-Tuning**: Unfreeze some layers (e.g., `for param in model.layer4.parameters(): param.requires_grad = True`) for higher accuracy.\n",
    "\n",
    "3. **Model Choice**:  \n",
    "   - ResNet18 is a good starting point. For better accuracy, try **EfficientNet** or **MobileNetV2**:\n",
    "     ```python\n",
    "     model = models.efficientnet_b0(pretrained=True)\n",
    "     model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
    "     ```\n",
    "\n",
    "4. **Expected Performance**:  \n",
    "   - With ResNet18 (feature extraction), expect **~90% accuracy** in 5 epochs.  \n",
    "   - Fine-tuning can push this to **~93%**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Full Code Example**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YH7wAlnWXpIK6VY-dV6s6QKJbJm5x1Wn?usp=sharing)  \n",
    "For a ready-to-run notebook, see the linked Colab above.\n",
    "\n",
    "Let me know if youâ€™d like to adapt another pretrained model (e.g., ViT, ConvNeXt)! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision Transformer (ViT)** and **EfficientNetV2** for **Fashion MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s how to adapt **Vision Transformer (ViT)** and **EfficientNetV2** (the latest stable version) for Fashion MNIST classification in PyTorch, including key modifications for grayscale images and performance optimizations:\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 1: Vision Transformer (ViT)**\n",
    "#### **Key Adaptations**:\n",
    "- ViT expects 3-channel input â†’ Convert grayscale to RGB.\n",
    "- ViT uses patch-based attention â†’ Adjust for Fashion MNISTâ€™s 28x28 â†’ 224x224 upscaling.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# 1. Data Prep (ViT-specific)\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # 1 â†’ 3 channels\n",
    "    transforms.Resize(224),  # ViT-B/16 expects 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Simpler norm\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=vit_transform)\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=vit_transform)\n",
    "\n",
    "# 2. Load Pretrained ViT\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():  # Freeze all\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier head\n",
    "model.heads.head = torch.nn.Linear(model.heads.head.in_features, 10)  # 10 classes\n",
    "\n",
    "# 3. Train (ViT converges faster with AdamW)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop (1-3 epochs often suffice for ViT)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for images, labels in torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 4. Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in torch.utils.data.DataLoader(test_data, batch_size=32):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "print(f\"ViT Test Accuracy: {100 * correct / len(test_data):.2f}%\")  # ~92-94%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: EfficientNetV2**\n",
    "#### **Key Adaptations**:\n",
    "- EfficientNetV2 also expects 3-channel input â†’ Grayscale to RGB.\n",
    "- Uses **AutoAugment**-style preprocessing (optional but recommended).\n",
    "\n",
    "```python\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "# 1. Data Prep (EfficientNetV2 prefers larger images)\n",
    "effnet_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(384),  # EfficientNetV2-S prefers 384x384\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=effnet_transform)\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=effnet_transform)\n",
    "\n",
    "# 2. Load Pretrained EfficientNetV2-S\n",
    "model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():  # Freeze all\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)  # 10 classes\n",
    "\n",
    "# 3. Train (Use smaller LR for EfficientNet)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop (3-5 epochs)\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for images, labels in torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 4. Evaluate\n",
    "correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in torch.utils.data.DataLoader(test_data, batch_size=32):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        correct += (model(images).argmax(1) == labels).sum().item()\n",
    "print(f\"EfficientNetV2 Test Accuracy: {100 * correct / len(test_data):.2f}%\")  # ~93-95%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "| Model           | Input Size | Expected Accuracy | Training Time (Epochs) | Best For                     |\n",
    "|-----------------|------------|-------------------|------------------------|-----------------------------|\n",
    "| **ViT-B/16**    | 224x224    | 92-94%            | 1-3                    | High-accuracy, GPU-friendly  |\n",
    "| **EfficientNetV2-S** | 384x384 | 93-95%       | 3-5                    | Balanced speed/accuracy      |\n",
    "\n",
    "### **Recommendations**\n",
    "1. **For GPU Users**: ViT converges faster (1-3 epochs) and achieves high accuracy.\n",
    "2. **For Edge Devices**: EfficientNetV2 is lighter and scales better to smaller resolutions.\n",
    "3. **Data Augmentation**: Add `RandomHorizontalFlip()` or `ColorJitter()` to improve robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Notes**\n",
    "- **ViT**: May overfit on small datasets (like Fashion MNIST) without augmentation.\n",
    "- **EfficientNetV2**: More parameter-efficient; better for production deployment.\n",
    "\n",
    "Let me know if youâ€™d like to explore hybrid approaches (e.g., ViT + CNN)! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hybrid approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three compelling **hybrid CNN-Transformer architectures** that blend the strengths of convolutional networks (local feature extraction) and vision transformers (global context modeling), along with their ideal use cases and implementation tips:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Convolutional Vision Transformer (CvT)**\n",
    "**Concept**:  \n",
    "Replaces ViTâ€™s linear patch embedding with **convolutional tokenization**, merging CNNâ€™s spatial hierarchy with Transformerâ€™s attention.  \n",
    "**Why Use It?**  \n",
    "- Better at capturing fine-grained local features than pure ViTs.  \n",
    "- More efficient than standard ViTs on small datasets (like Fashion MNIST).  \n",
    "\n",
    "**Implementation Snippet**:  \n",
    "```python\n",
    "from transformers import CvtModel, CvtConfig\n",
    "\n",
    "config = CvtConfig(image_size=224, num_channels=3)  # Convert grayscale to 3-channel\n",
    "model = CvtModel(config).from_pretrained(\"microsoft/cvt-13\")\n",
    "model.head = torch.nn.Linear(model.head.in_features, 10)  # Replace classifier\n",
    "```\n",
    "\n",
    "**Best For**:  \n",
    "- Tasks needing both local texture details (e.g., fabric patterns in Fashion MNIST) and global structure.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. MobileViT (Lightweight Hybrid)**\n",
    "**Concept**:  \n",
    "Uses **mobile-friendly convolutions** paired with lightweight transformers, optimized for edge devices.  \n",
    "**Why Use It?**  \n",
    "- 2â€“3x faster than ViTs on mobile CPUs.  \n",
    "- Retains CNNâ€™s spatial inductive bias while adding global attention.  \n",
    "\n",
    "**Implementation Snippet**:  \n",
    "```python\n",
    "from torchvision.models import mobilevit_s\n",
    "model = mobilevit_s(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 10)\n",
    "```\n",
    "\n",
    "**Best For**:  \n",
    "- Real-time applications (e.g., mobile Fashion MNIST classification).  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. MaxViT (Multi-Axis Hybrid)**\n",
    "**Concept**:  \n",
    "Combines **convolutional blocks** with **hierarchical transformers**, using both local and global attention.  \n",
    "**Why Use It?**  \n",
    "- State-of-the-art on ImageNet with minimal compute overhead.  \n",
    "- Scales seamlessly from low to high resolution.  \n",
    "\n",
    "**Implementation Snippet**:  \n",
    "```python\n",
    "from maxvit import maxvit_tiny  # Requires `pip install maxvit`\n",
    "model = maxvit_tiny(pretrained=True)\n",
    "model.head.fc = torch.nn.Linear(model.head.fc.in_features, 10)\n",
    "```\n",
    "\n",
    "**Best For**:  \n",
    "- High-resolution datasets where both fine details (e.g., clothing seams) and global trends (outfit style) matter.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Choose Which?**\n",
    "| Hybrid Model       | Strengths                          | Ideal Use Case                     |\n",
    "|--------------------|-----------------------------------|-----------------------------------|\n",
    "| **CvT**           | Balanced local/global features    | Small datasets with fine details  |\n",
    "| **MobileViT**     | Edge-optimized, fast inference    | Mobile/embedded deployment        |\n",
    "| **MaxViT**        | SOTA accuracy, scalable           | High-res or multi-scale images    |\n",
    "\n",
    "**Next Steps**:  \n",
    "Let me know which hybrid youâ€™d like to implement fully (Iâ€™ll provide a complete training pipeline)! For example:  \n",
    "- **MobileViT** for a lightweight Fashion MNIST app.  \n",
    "- **MaxViT** for maximal accuracy.  \n",
    "\n",
    "Hybrids excel when pure CNNs lack global context or pure ViTs miss local details. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
